{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_catboost.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "#drive.mount('/content/gdrive')\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras import layers, models, optimizers, Sequential\n",
        "from sklearn.metrics import accuracy_score, log_loss, make_scorer\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "train = pd.read_csv('/content/gdrive/MyDrive/dacon_exercise/train_features.csv')\n",
        "test = pd.read_csv('/content/gdrive/MyDrive/dacon_exercise/test_features.csv')\n",
        "label = pd.read_csv('/content/gdrive/MyDrive/dacon_exercise/train_labels.csv')\n",
        "\n",
        "# add the feature(total energy of acc)\n",
        "train['acc_t']  = train.apply(lambda x : (x['acc_x']**2 + x['acc_y'] **2 +  x['acc_z'] ** 2 )**(1/3), axis=1)\n",
        "test['acc_t']  = test.apply(lambda x : (x['acc_x']**2 + x['acc_y'] **2 +  x['acc_z'] ** 2 )**(1/3), axis=1)\n",
        "\n",
        "# convert the data to 1d (600, 1, 7) shape\n",
        "def convert_1d(data):\n",
        "    colname = data.columns\n",
        "    converted = np.dstack((data.loc[:, colname[0]], data.loc[:, colname[1]],data.loc[:, colname[2]], data.loc[:, colname[3]], data.loc[:, colname[4]], data.loc[:, colname[5]], data.loc[:, colname[6]]))\n",
        "    return converted\n",
        "\n",
        "\n",
        "def data_to_img(data):\n",
        "    train_sub = []\n",
        "    \n",
        "\n",
        "    for i in range(len(data)//600):\n",
        "        data_sub = data.iloc[i*600:(i+1)*600, 2:].reset_index(drop=True)\n",
        "        data_sub_converted = convert_1d(data_sub)\n",
        "        train_sub.append(data_sub_converted)\n",
        "\n",
        "    return np.array(train_sub).reshape(len(train_sub),600, 7)\n",
        "\n",
        "train_img = data_to_img(train)\n",
        "test_img = data_to_img(test)\n",
        "\n",
        "# stratified split of train data for class imbalance\n",
        "strat = StratifiedShuffleSplit(n_splits=1, test_size = 0.2, random_state=1004)\n",
        "for train_index, valid_index in strat.split(train_img, label['label']):\n",
        "    x_train, x_valid = train_img[train_index], train_img[valid_index]\n",
        "    y_train, y_valid = label_onehot[train_index], label_onehot[valid_index]\n",
        "    \n",
        "# residual block\n",
        "def Residual_Block(x, n_ch, kernel_size, leaky_alpha):\n",
        "    skip_connection = x\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU(alpha=leaky_alpha)(x)\n",
        "    x = layers.Conv1D(n_ch, kernel_size=kernel_size, strides=1, padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU(alpha = leaky_alpha)(x)\n",
        "    x = layers.Conv1D(n_ch, kernel_size=kernel_size, strides=1, padding='same')(x)\n",
        "    x = layers.add([x, skip_connection])\n",
        "    \n",
        "    return x\n",
        "# make the hypermodel for 1d-resnet\n",
        "from kerastuner import HyperModel, Objective\n",
        "import tensorflow as tf\n",
        "from kerastuner.tuners import BayesianOptimization\n",
        "import keras.backend as K\n",
        "\n",
        "class MyHyperModel(HyperModel):\n",
        "    \n",
        "    def build(self, hp):\n",
        "        inputs = layers.Input(shape=(600,7), name='inputs')\n",
        "        filter = hp.Int('num_filters',min_value = 32, max_value = 128, step = 32, default = 64)\n",
        "        x = layers.Conv1D(filters = filter, kernel_size = hp.Choice('kernel1', [1, 3, 5, 7]), padding = 'valid')(inputs)\n",
        "        x = layers.AveragePooling1D(2)(x)\n",
        "        num_layers = hp.Int('num_layers', 1, 3)\n",
        "        for i in range(num_layers):\n",
        "            kernel_size = hp.Int('kernel2', 1, 7, default = 3)\n",
        "            x = Residual_Block(x, filter, kernel_size = hp.Choice('kernel3', [1, 3, 5, 7]),leaky_alpha = hp.Float('alpha1', 0.0, 1.0, step = 0.01, default = 0.03))\n",
        "            x = Residual_Block(x, filter, kernel_size = hp.Choice('kernel4', [1, 3, 5, 7]),leaky_alpha = hp.Float('alpha2', 0.0, 1.0, step = 0.01, default = 0.03))\n",
        "            x = layers.BatchNormalization()(x)\n",
        "            \n",
        "            filter = filter*2\n",
        "\n",
        "            if i != num_layers-1:\n",
        "                x = layers.Conv1D(filter, kernel_size = hp.Choice('kernel5', [1, 3, 5, 7]), strides=1, padding='same')(x)\n",
        "            else:\n",
        "                x = x\n",
        "\n",
        "        x = layers.GlobalAveragePooling1D(name = 'GlobalAveragePooling')(x)\n",
        "        output = layers.Dense(61, activation='softmax')(x)\n",
        "\n",
        "        model = models.Model([inputs],[output])\n",
        "        model.compile(optimizer = optimizers.Adam(learning_rate=hp.Float('lr', 0.001, 0.01, step = 0.001)), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "        return model\n",
        "\n",
        "hypermodel = MyHyperModel()\n",
        "\n",
        "from keras.callbacks import EarlyStopping\n",
        "es = EarlyStopping(monitor = 'val_loss', patience = 4)\n",
        "\n",
        "tuner_BO = BayesianOptimization(\n",
        "            hypermodel,\n",
        "            objective='val_loss',\n",
        "            max_trials=50,\n",
        "            seed=42,\n",
        "            executions_per_trial=2,\n",
        "        )\n",
        "tuner_BO.search(x_train, y_train, epochs=50, validation_data=(x_valid, y_valid), verbose=1, callbacks = [es], batch_size = 15)\n",
        "best_model = tuner_BO.get_best_models(num_models=1)[0]\n",
        "best_model.evaluate(x_valid, y_valid)\n",
        "\n",
        "best_model.save('/content/gdrive/MyDrive/dacon_exercise/resnet_7channel_BO.h5')\n",
        "\n",
        "# define the CNN model which has the output of layer GlobalAvergaPooling(get the 128 features from CNN)\n",
        "best_model = models.load_model('/content/gdrive/MyDrive/dacon_exercise/resnet_7channel_BO.h5')\n",
        "model_global = models.Model(best_model.input, best_model.get_layer('GlobalAveragePooling').output)\n",
        "train_global = model_global.predict(train_img)\n",
        "test_global = model_global.predict(test_img)\n",
        "\n",
        "# gridsearch for catboost\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "clf = CatBoostClassifier(task_type='GPU', border_count=None)\n",
        "params = {'iterations': [600, 700, 800, 900, 1000],\n",
        "          'depth': [4, 5, 6],\n",
        "          'loss_function': ['MultiClass'],\n",
        "          'l2_leaf_reg': np.logspace(-20, -19, 3),\n",
        "          'leaf_estimation_iterations': [10],\n",
        "           'eval_metric': ['Accuracy'],\n",
        "          'logging_level':['Silent'],\n",
        "          'random_seed': [42]\n",
        "         }\n",
        "\n",
        "from sklearn.model_selection import PredefinedSplit\n",
        "split_index = [-1 if x in train_index else 0 for x in np.arange(3125)]\n",
        "pds = PredefinedSplit(test_fold = split_index)\n",
        "\n",
        "from sklearn.metrics import log_loss, make_scorer\n",
        "LogLoss = make_scorer(log_loss, greater_is_better=False, needs_proba=True)\n",
        "\n",
        "clf_grid = GridSearchCV(estimator=clf, param_grid=params, scoring=LogLoss, cv=pds, verbose = 10)\n",
        "clf_grid.fit(X = train_global, y = label['label'])\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "cat = CatBoostClassifier(task_type = 'GPU', loss_function='MultiClass', depth=4, eval_metric='Accuracy', iterations=800, l2_leaf_reg=1e-20, leaf_estimation_iterations=10, random_seed=42, verbose = 10)\n",
        "\n",
        "for train_index, valid_index in strat.split(train_img, label['label']):\n",
        "    x_train, x_valid = train_global[train_index], train_global[valid_index]\n",
        "    y_train, y_valid = label['label'][train_index], label['label'][valid_index]\n",
        "\n",
        "cat.fit(x_train, y_train)\n",
        "\n",
        "# submission file\n",
        "id = pd.DataFrame(test['id'].unique(), columns = ['id'])\n",
        "test_pred = cat.predict_proba(test_global)\n",
        "test_pred = pd.concat([id, pd.DataFrame(test_pred)], axis = 1)\n",
        "test_pred.to_csv('/content/gdrive/MyDrive/dacon_exercise/resnet_7channel_BO_cat_2.csv', index = False)\n"
      ],
      "metadata": {
        "id": "kwwnAnWS6Xop"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}